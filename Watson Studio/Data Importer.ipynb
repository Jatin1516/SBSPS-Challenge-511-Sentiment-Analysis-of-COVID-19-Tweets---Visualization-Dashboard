{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Twitter Data Handeler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1. Data Importing\n",
    "The dataset would be imported here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:: The dataset is for non comercial purpose and was downloaded from [here](https://ieee-dataport.org/open-access/coronavirus-covid-19-tweets-dataset). I own no rights to this dataset and am not responsible for any missuse of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'botocore'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5a0f39171c1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mibm_boto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'botocore'"
     ]
    }
   ],
   "source": [
    "import types\n",
    "import json\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "client_2bfc56ae673c4e3aad4c3da569258436 = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='XXX',\n",
    "    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3.eu-geo.objectstorage.service.networklayer.com')\n",
    "\n",
    "def fetcher(number):\n",
    "    global client_2bfc56ae673c4e3aad4c3da569258436\n",
    "    body = client_2bfc56ae673c4e3aad4c3da569258436.get_object(Bucket='sentimentanalysisproject-donotdelete-pr-pkfzekostvak36',Key='corona_tweets_' + str(number).zfill(2) + '.csv')['Body']\n",
    "    # add missing __iter__ method, so pandas accepts body as file-like object\n",
    "    if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "    data = pd.read_csv(body, names=['tweet_id', 'sentiment_score'], )\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = pd.concat([fetcher(num) for num in range(1, 4)], ignore_index=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text Hydration and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install twarc    #A tool used to hydrate twitter text from tweet_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isIndian(loc):#checks id location is Indian\n",
    "    try:\n",
    "        if (loc == 'IN') or (loc[-5:] == 'India'):\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carveData(l, tweet, ss):#Will be used to carve the required data from a tweet\n",
    "    location = None\n",
    "    try:\n",
    "        location = tweet['user']['location']\n",
    "    except:\n",
    "        pass\n",
    "    if not location:\n",
    "        try:\n",
    "            location = tweet['place']['full_name']\n",
    "        except:\n",
    "            pass\n",
    "    if not location:\n",
    "        try:\n",
    "            location = tweet['retweeted_status']['user']['location']\n",
    "        except:\n",
    "            pass\n",
    "    if not location:\n",
    "        try:\n",
    "            location = tweet['retweeted_status']['place']['full_name']\n",
    "        except:\n",
    "            pass\n",
    "    if not location:\n",
    "        try:\n",
    "            location = tweet['place']['country_code']\n",
    "        except:\n",
    "            pass\n",
    "    if not location:\n",
    "        try:\n",
    "            location = tweet['retweeted_status']['place']['country_code']\n",
    "        except:\n",
    "            pass\n",
    "    if not location:\n",
    "        return location\n",
    "    if not isIndian(location):\n",
    "        return\n",
    "    hashtags = []\n",
    "    try:\n",
    "        for ht in tweet['entities']['hashtags']:\n",
    "            hashtags.append(ht['text'])\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        for ht in tweet['retweeted_status']['entities']['hashtags']:\n",
    "            hashtags.append(ht['text'])\n",
    "    except:\n",
    "        pass\n",
    "    user_mentions = []\n",
    "    try:\n",
    "        for n in tweet['entities']['user_mentions']:\n",
    "            user_mentions.append(n['name'])\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        for n in tweet['retweeted_status']['entities']['user_mentions']:\n",
    "            user_mentions.append(n['name'])\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        user_mentions.append(tweet['retweeted_status']['user']['name'])\n",
    "    except:\n",
    "        pass\n",
    "    text = None\n",
    "    try:\n",
    "        text = tweet['text']\n",
    "    except:\n",
    "        pass\n",
    "    created_at = None\n",
    "    try:\n",
    "        created_at = tweet['created_at']\n",
    "    except:\n",
    "        pass\n",
    "    _id = None\n",
    "    try:\n",
    "        _id = tweet['id']\n",
    "    except:\n",
    "        pass\n",
    "    k = {'id': _id, 'sentiment_score': ss, 'created_at': created_at, 'user_mentions': user_mentions, 'hashtags': hashtags, 'location': location, 'text': text}\n",
    "    l.append(k)\n",
    "    return _id\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twarc import Twarc\n",
    "\n",
    "consumer_key='XXX'\n",
    "consumer_secret='XXX'\n",
    "access_token='XXX'\n",
    "access_token_secret='XXX'\n",
    "\n",
    "t = Twarc(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "\n",
    "#list of important Factors\n",
    "#l[0]['user']['location'][-5:] == 'India'\n",
    "#l[0]['entities']['hashtags']\n",
    "#l[0]['place']['country_code'] == 'IN'\n",
    "#l[0]['place']['full_name']\n",
    "#l[0]['retweeted_status']\n",
    "#for x in l[0]['entities']['user_mentions']:\n",
    "#    x['name']\n",
    "#l[0]['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l =[]\n",
    "i = 0\n",
    "for index, row in data.iterrows():\n",
    "    for tw in t.hydrate([int(row['tweet_id'])]):\n",
    "        tweet_id = carveData(l, tw, row['sentiment_score'])\n",
    "        i+=1\n",
    "    print('\\rObjects :: ' + str(len(l)) + '/' + str(i), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(l, columns=['id', 'sentiment_score', 'created_at', 'user_mentions', 'hashtags', 'location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\n",
    "from project_lib import Project\n",
    "project = Project(project_id='XXX', project_access_token='XXX')\n",
    "pc = project.project_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save_data(data=new_df.to_csv(),file_name='processed_data_01_03.csv',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Categorization\n",
    "Here the dataset would be divided into various parts based on the timeline of actions taken by the Indian gov. The categories are listed below:\n",
    "   1. Before Janata Lockdown\n",
    "   2. Durring Janata Lockdown (22 March from 7 a.m. to 9 p.m.)\n",
    "   3. Phase 1 (25 March – 14 April)\n",
    "   4. Phase 2 (15 April – 3 May)\n",
    "   5. Phase 3 (4–17 May)\n",
    "   6. Phase 4 (18–31 May)\n",
    "   7. Phase 5 / Unlock 1 (1–30 June) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-1,0) is considered negative sentiment, 0 is considered neutral and (0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tw for tw in t.hydrate([1240861450941247489])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(1240861450941247489)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, d in zip(l, data):\n",
    "    print(d[0], d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1240861450941247489, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=['tweet_id', 'ss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = range(2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isIndian(tw['user']['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.DataFrame([], columns=['tweet_id', 'sentiment_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "    print(int(row['tweet_id']), row['sentiment_score'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
